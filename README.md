# Exno-2-Prompt-Engg
# Ex.No: 2 Evaluation of 2024 Prompting Tools Across Diverse AI Platforms:
ChatGPT, Claude, Bard, Cohere Command, and Meta

REGISTER NUMBER : 212223060113
Name : Karnatam Bindu
# Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.

# Algorithm:
Define the Use Case: Select a specific task for evaluation across platforms (e.g., summarizing a document, answering a technical question, or generating a creative story / Code). Ensure the use case is applicable to all platforms and will allow for comparison across response quality, accuracy, and depth. Create a Set of Prompts: Prepare a uniform set of prompts that align with the chosen use case. Each prompt should be clear and precise, ensuring that all platforms are evaluated using the same input. Consider multiple prompts to capture the versatility of each platform in handling different aspects of the use case. Run the Experiment on Each AI Platform: Input the prompts into each AI tool (ChatGPT, Claude, Bard, Cohere Command, and Meta) and gather the responses. Ensure the same conditions are applied for each platform, such as input format, time to respond, and prompt delivery. Record response times, ease of interaction with the platform, and any technical issues encountered. Evaluate Response Quality: Assess each platform’s responses using the following criteria: Accuracy,Clarity,Depth,Relevance Compare Performance: Compare the collected data to identify differences in performance across platforms. Identify any platform-specific advantages, such as faster response times, more accurate answers, or more intuitive interfaces. Deliverables: A comparison table outlining the performance of each platform (ChatGPT, Claude, Bard, Cohere Command, and Meta) based on accuracy, clarity, depth, and relevance of responses. A final report summarizing the findings of the experiment, including recommendations on the most suitable AI platform for different use cases based on performance and user

# Introduction:
Objective: Compare the performance, user experience, and response quality of AI platforms in a specific use case. Use Case: [e.g., Summarization, Technical Q&A, Creative Writing] AI Platforms Tested: ChatGPT, Claude, Gemini, Meta

# Experiment Setup:
Use Case: [Summarization / Technical Q&A / Creative Writing] Evaluation Criteria: Accuracy, Response Time, UX (User Experience)

# Prompt 1(Summarization):
"Summarize the following passage in 2-3 sentences: Artificial Intelligence (AI) has rapidly transformed various industries, from healthcare to finance. In healthcare, AI-powered diagnostic tools are improving early disease detection, reducing errors, and assisting doctors in making data-driven decisions. Similarly, in finance, AI-driven algorithms optimize trading strategies, detect fraudulent activities, and enhance customer experiences through personalized services. However, ethical concerns such as bias in AI models, data privacy, and job displacement continue to be challenges that need careful regulation. Experts emphasize the importance of transparency, accountability, and continuous evaluation to ensure AI benefits society without unintended consequences."

# Prompt 2(Technical Q&A):
Briefly explain how AI helps in fraud detection and its potential risks (limit: 3 sentences).

# Prompt 3(Creative Writing):
Write a short paragraph (maximum 3 sentences) from the perspective of an AI struggling with emotional intelligence.

# Response Analysis:
# ChatGPT Evaluation
Criteria Rating (1-5)

Accuracy (Is the answer correct and relevant?) 4/ 5

Response Time (Was the answer given quickly?) 4 / 5

UX (User Experience) (Was the interaction smooth and useful?) 4 / 5

Overall Score (Total) 12/ 15

# Claude Evaluation
Criteria Rating (1-5)

Accuracy 3/ 5

Response Time 5/ 5

UX (User Experience) 5 / 5

Overall Score (Total) 13/ 15

# Gemini Evaluation
Criteria Rating (1-5)

Accuracy 3/ 5

Response Time 3 / 5

UX (User Experience) 3.5 / 5

Overall Score (Total) 10 / 15

# Meta Evaluation
Criteria Rating (1-5)

Accuracy 4/ 5

Response Time 3/ 5

UX (User Experience) 4 / 5

Overall Score (Total) 11 / 15

# Final Winner
Best Overall AI: Claude
Reason: The User Experience was good and colour full compared to others

# AI outputs:
1: Prompt 1(Summarization): "Summarize the following passage in 2-3 sentences:
Artificial Intelligence (AI) has rapidly transformed various industries, from healthcare to finance. In healthcare, AI-powered diagnostic tools are improving early disease detection, reducing errors, and assisting doctors in making data-driven decisions. Similarly, in finance, AI-driven algorithms optimize trading strategies, detect fraudulent activities, and enhance customer experiences through personalized services. However, ethical concerns such as bias in AI models, data privacy, and job displacement continue to be challenges that need careful regulation. Experts emphasize the importance of transparency, accountability, and continuous evaluation to ensure AI benefits society without unintended consequences."

 Chat CPT:
<img width="1181" height="207" alt="Screenshot 2025-09-06 222205" src="https://github.com/user-attachments/assets/c0fd08c6-46f1-4f1b-bf1c-3cdb42174f20" />

Claude:
<img width="1048" height="396" alt="Screenshot 2025-09-06 222413" src="https://github.com/user-attachments/assets/222516e7-0276-4473-8c9e-bfa865e17047" />


Gemini:
<img width="1057" height="339" alt="Screenshot 2025-09-06 222521" src="https://github.com/user-attachments/assets/a276b9e0-cea8-4da3-9837-0057e57fd7dd" />


Meta:
<img width="1035" height="172" alt="image" src="https://github.com/user-attachments/assets/4366023e-9c80-4e2c-b588-ea7ee7d1d295" />


2: Prompt 2(Technical Q&A): Briefly explain how AI helps in fraud detection and its potential risks (limit: 3 sentences).
Chat GPT:
<img width="1049" height="231" alt="Screenshot 2025-09-06 223514" src="https://github.com/user-attachments/assets/ae929f83-f0b9-4c21-a42b-d4a5d6e759ee" />

Claude:
<img width="1057" height="372" alt="Screenshot 2025-09-06 223527" src="https://github.com/user-attachments/assets/00f1a91a-33c1-411b-aa1a-139d20fc5932" />

Gemini:
<img width="1095" height="284" alt="Screenshot 2025-09-06 223535" src="https://github.com/user-attachments/assets/008a7be0-a132-496c-8c7b-3ae58c97d593" />

Meta:
<img width="1035" height="172" alt="Screenshot 2025-09-06 223556" src="https://github.com/user-attachments/assets/21262d3f-4d14-4637-8d37-b55c8accf504" />



3: Prompt 3(Creative Writing): Write a short paragraph (maximum 3 sentences) from the perspective of an AI struggling with emotional intelligence.
Chat GPT:
<img width="1065" height="199" alt="Screenshot 2025-09-06 224033" src="https://github.com/user-attachments/assets/f57faab0-f221-4bd1-92f1-cf1d5d73442f" />



Claude:

<img width="1092" height="591" alt="Screenshot 2025-09-06 224046" src="https://github.com/user-attachments/assets/0360dc28-4c88-444d-8e24-39f2bc5ab9bc" />


Gemini:
<img width="1096" height="215" alt="Screenshot 2025-09-06 224059" src="https://github.com/user-attachments/assets/0165aaf9-912d-4e04-b01f-36726497846d" />



Meta:
<img width="949" height="176" alt="Screenshot 2025-09-06 224116" src="https://github.com/user-attachments/assets/4c95ef47-e81f-4382-a9c8-2e9ff1812f78" />



Analysis & Discussion
ChatGPT (GPT-4):
This platform excelled in all areas, particularly clarity and relevance. The summary was both concise and accurate, with just the right amount of detail for college students. Its strengths make it ideal for educational use, where students need clear and precise information without over-simplification.

Claude (Claude 3):
Claude performed well, though slightly less accurate than ChatGPT in its explanations. It did, however, maintain strong clarity and relevance, which could make it suitable for educational contexts, albeit with slightly less depth than ChatGPT.

Bard (Gemini 1.5):
Bard’s summary was quick and simple, but it lacked depth and technical coverage. This makes it more suitable for quick summaries but not for detailed educational use where understanding the underlying concepts is key.

Cohere Command R+:
Cohere’s summary was overly simplified, lacking the technical depth needed for a comprehensive understanding. This tool might be best for general audiences or for contexts where extreme simplicity is required, but not for technical education.

Meta (LLaMA 2):
Meta’s output was vague and lacked both depth and clarity. It scored poorly across all metrics and should likely be avoided for technical summarization tasks in educational settings.

<img width="1075" height="807" alt="Screenshot 2025-09-06 224419" src="https://github.com/user-attachments/assets/5b893162-ce90-4c7d-bbaf-f165c0141094" />


# Conclusion & Insights:
# Best Performing AI: Claude
Claude emerged as the best-performing AI platform in this evaluation, offering the most balanced experience across accuracy, response time, and user experience. While ChatGPT and Meta showed strong accuracy, Claude’s quick responses and smooth, user-friendly interface gave it the edge overall. Gemini performed modestly, with room for improvement in both speed and content quality. Ultimately, Claude is recommended for users seeking an all-around reliable AI, while those prioritizing technical accuracy might prefer ChatGPT or Meta.

# Result:
Thus the Prompting tools are executed and analysed sucessfully .
